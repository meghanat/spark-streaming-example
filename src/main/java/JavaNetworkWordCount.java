import java.util.Arrays;
import java.util.regex.Pattern;
import scala.Tuple2;
import java.io.FileReader;  
import java.io.BufferedReader;
import java.util.ArrayList;
import java.util.LinkedList;
import java.util.List;
import java.util.Queue;



import org.apache.spark.SparkConf;
import org.apache.spark.api.java.StorageLevels;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.log4j.Logger;
import org.apache.log4j.Level;



/**
 * Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
 *
 * Usage: JavaNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.JavaNetworkWordCount localhost 9999`
 */
public final class JavaNetworkWordCount {
  private static final Pattern SPACE = Pattern.compile(" ");


  // public static void main(String[] args) throws Exception {
  //   if (args.length < 2) {
  //     System.err.println("Usage: JavaNetworkWordCount <hostname> <port>");
  //     System.exit(1);
  //   }

  //   // StreamingExamples.setStreamingLogLevels();
  //   Logger.getLogger("org").setLevel(Level.ERROR);
  //   Logger.getLogger("akka").setLevel(Level.ERROR);
  //   HelloWorld h = new HelloWorld();
  //   h.sayHi("JNI");
  //   h.printPID();
  //   h.printSchedType();
  //   // h.setSchedType();
  //   h.printSchedType();

  //   // Create the context with a 1 second batch size
  //   SparkConf sparkConf = new SparkConf().setAppName("JavaNetworkWordCount");
  //   JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, Durations.seconds(1));

  //   // String dataDirectory = "remoteFolder/streaming";
  //   // JavaDStream<String> lines =ssc.textFileStream(dataDirectory);

  //   // Create a JavaReceiverInputDStream on target ip:port and count the
  //   // words in input stream of \n delimited text (eg. generated by 'nc')
  //   // Note that no duplication in storage level only for running locally.
  //   // Replication necessary in distributed scenario for fault tolerance.
  //   JavaReceiverInputDStream<String> lines = ssc.socketTextStream(
  //           args[0], Integer.parseInt(args[1]), StorageLevels.MEMORY_AND_DISK_SER);
  //   JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(SPACE.split(x)).iterator());
  //   JavaPairDStream<String, Integer> wordCounts = words.mapToPair(s -> new Tuple2<>(s, 1))
  //       .reduceByKey((i1, i2) -> i1 + i2);

  //   wordCounts.print();
  //   // // wordCounts.saveAsTextFiles("/home/hduser/data/output/",".txt");
  //   //  wordCounts.foreachRDD(rdd ->{
  //   //   if(!rdd.isEmpty()){
  //   //      rdd.coalesce(1).saveAsTextFile("/home/hduser/data/output/");
  //   //   }
  //   // });
  //   ssc.start();
  //   ssc.awaitTermination();
  // }
  public static void main(String[] args) throws Exception {

    // StreamingExamples.setStreamingLogLevels();
    SparkConf sparkConf = new SparkConf().setAppName("JavaQueueWordCount");

    // Create the context
    JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, Durations.seconds(1));


    // Create the queue through which RDDs can be pushed to
    // a QueueInputDStream

    // Create and push some RDDs into the queue
    // List<Integer> list = new ArrayList<>();
    // for (int i = 0; i < 1000; i++) {
    //   list.add(i);
    // // }
    // List<String> list = new ArrayList<>();
    // try (BufferedReader br = new BufferedReader(new FileReader("/home/hduser/data/alice"))) {
    //   String line;
    //   while ((line = br.readLine()) != null) {
    //     list.add(line);
    //   }
    // }

    // Queue<JavaRDD<String>> rddQueue = new LinkedList<>();
    // for (int i = 0; i < 30; i++) {
    //   rddQueue.add(ssc.sparkContext().parallelize(list));
    // }

    // Create the QueueInputDStream and use it do some processing
    JavaDStream<String> inputStream = ssc.textFileStream("/home/hduser/data/");
    JavaDStream<String> words = inputStream.flatMap(x -> Arrays.asList(SPACE.split(x)).iterator());
    JavaPairDStream<String, Integer> wordCounts = words.mapToPair(s -> new Tuple2<>(s, 1))
        .reduceByKey((i1, i2) -> i1 + i2);

    wordCounts.print();
    ssc.start();
    ssc.awaitTermination();
  }

}
